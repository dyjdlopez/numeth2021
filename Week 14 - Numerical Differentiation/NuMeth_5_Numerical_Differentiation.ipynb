{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NuMeth 5- Numerical Differentiation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMJcgB5IxXLMq4m6XpvfD3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyjdlopez/numeth2021/blob/main/Week%2014%20-%20Numerical%20Differentiation/NuMeth_5_Numerical_Differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC5cBEnfjxrf"
      },
      "source": [
        "# Numerical Differentiation\n",
        "$_{\\text{Â©D.J. Lopez | 2021 | Computational Methods for Computer Engineers}}$\n",
        "\n",
        "Extending the topic of optimization, differention is vital in finding gradients of equations. In your Calculus classes, you were taught to solve derivatives symbolically. Although that is also possible in Python, we will focus on Numerical Differentiation since our course focuses on numerical techniques. Numerical differentiation differs from symbolic differentiation since numerical differentiation requires to get the images of the function for certain number of steps. In this module, we will learn how this works and how you can solve them computationally in Python. This module will cover:\n",
        "* Forward Finite Derivatives\n",
        "* Central Fininte Derivatives\n",
        "* Backward Fininte Derivatives\n",
        "* Introduction to the Taylor Series Expansion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS8UMN5ahj8W"
      },
      "source": [
        "## Review\n",
        "If you recall, the first discussion of derivatives starts with the increment method rather than the power rule or refering to the tables. \n",
        "\n",
        "![image](https://s3-us-west-2.amazonaws.com/courses-images/wp-content/uploads/sites/2332/2018/01/11205220/CNX_Calc_Figure_03_02_001.jpg)\n",
        "\n",
        "Referring to the image above, it represents a graph of the equation of $f(x)$. Other elements such as $a$ represents a certain input. The orange line is identified as the gradient of the function $f(x)$. The gradient can be computed given the general formula of the increment method:\n",
        "$$\\frac{\\Delta y}{\\Delta x} = \\frac{f(a+\\Delta x)-f(a)}{\\Delta x} \\\\ _{\\text{(Eq. 5.1)}}$$ \n",
        "The gradient is simply the ratio of the changes in $y$ to the change of $x$, or simply the slope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y0DPdJqkkck"
      },
      "source": [
        "## 5.1 Forward Finite Differentiation\n",
        "\n",
        "Differentiation, from the word itself is taking differences. Here we are taking the differences of the function given an interval. The forward finite differentiation method is taken from Eq. 5.1. The concept behind the forward finite differentiation is we take steps or samples of the function by some $n$ number of increments from the input $x$. The sample $n$ is determined through the degree of the derivaitve we want. The formulae below shows the progression.\n",
        "$$f'(x) = \\frac{f(x_{i+1})-f(x_i)}{\\Delta x} \\\\\n",
        "f''(x) = \\frac{f(x_{i+2})-2f(x_{i+1})+f(x_i)}{\\Delta x^2} \\\\\n",
        "f'''(x) = \\frac{f(x_{i+3})-3f(x_{i+2})+3f(x_{i+1})-f(x_i)}{\\Delta x^3} \\\\\n",
        "f^{IV}(x) = \\frac{f(x_{i+4})-4f(x_{i+3})+6f(x_{i+2})-4f(x_{i+1})+f(x_i)}{\\Delta x^4} \\\\ _{\\text{(Eq. 5.2)}}\n",
        "$$\n",
        "\n",
        "Differentiation techniques will have a certain degree of approximation error. Due to the truncation from the Taylor series expansion. This is determined by the function $O(h)$ wheras $h$ is also $\\Delta x$. For forward finite differention is $O(\\Delta x)$.\n",
        "\n",
        "We shall try to compute for the first an second derivative of an equation at $x = 0.15$ with a $\\Delta x = 0.05$:\n",
        "$$f(x) = 4x^3+2x^2-x+1 \\\\ \n",
        "f'(x) = 12x^2 + 4x -1 \\\\\n",
        "f''(x) = 24x + 4$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w29kIgtapgmk"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWMEk0SIoF71"
      },
      "source": [
        "x = 0.1\n",
        "dx = 0.05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I5XWzzQo1v1"
      },
      "source": [
        "### Set the function and compute for the theoretical values\n",
        "f = lambda x : 4*x**3 + 2*x**2 - x +1\n",
        "f_1 = lambda x: 12*x**2 + 4*x -1\n",
        "f_2 = lambda x: 24*x + 4\n",
        "print(f'f(0.1) = {f(x)}')\n",
        "print(f'f\\'(0.1) = {f_1(x)}')\n",
        "print(f'f\\'\\'(0.1) = {f_2(x)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Imtj9RLpkKv"
      },
      "source": [
        "# Forward Definite Differentiation\n",
        "grad1 = (f(x+dx)-f(x))/dx\n",
        "grad2 = (f(x+2*dx)-2*f(x+dx)+f(x))/dx**2\n",
        "print(f'f(0.1) = {f(x)}')\n",
        "print(f'f\\'(0.1) = {grad1}, error @ {abs(f_1(x)-grad1)}')\n",
        "print(f'f\\'\\'(0.1) = {grad2}, error @ {abs(f_2(x)-grad2)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzoipka-qke0"
      },
      "source": [
        "### Making a general computational method\n",
        "Using the progression shown in Eq.5.2 we can develop a forward finite function. The pattern of the coefficients of the differences of the functions at every $\\Delta x$ can also be observed in the Pascal's Triangle.\n",
        "![image](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Pascal_triangle.svg/1200px-Pascal_triangle.svg.png)\n",
        "Each row of the triangle can be computed using the combinations formula or getting the coefficients from the binomial expansion. This can be achieved through encoding the Binomial Theorem, however we will use the `scipy.special.binom` function for brevity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4xNwaEcjsRa"
      },
      "source": [
        "from scipy.special import binom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RizboJknk5KF"
      },
      "source": [
        "binom_coeffs = lambda n : np.asarray([binom(n,k) for k in range(n+1)])\n",
        "binom_coeffs(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-9spFcwmJzR"
      },
      "source": [
        "## Making a Pascal Triangle\n",
        "for i in range(5):\n",
        "  print(binom_coeffs(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHurU_sSrha-"
      },
      "source": [
        "def diff_fwd(f,x,dx,degree=2):\n",
        "  f_ans = f(x+((np.arange(degree,-1,-1))*dx)) #get the increments of f(x)\n",
        "  bin = binom_coeffs(degree)\n",
        "  bin[1::2] *= -1 ## Changing the signs of the binomial coeffs following Eq.5.2\n",
        "  diff = (bin @ f_ans) / dx**degree #vecotrized form, since Eq.5.2 follows the linear combination form\n",
        "  return diff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6tWJ1A7Dw9c"
      },
      "source": [
        "degree = 3\n",
        "for n in range(0,degree+1):\n",
        "  print(diff_fwd(f,x,dx,n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj4j1C4OuPms"
      },
      "source": [
        "## 5.2 Backward Finite Differentiation\n",
        "\n",
        "The diffrence between the forward and backward finite differentiation is how $\\Delta y$ is sample. Instead of getting values incrementing from $x$ we take the values preceeding $x$ or simply doing decrements. The formulae below shows the progression.\n",
        "$$f'(x) = \\frac{f(x_{i})-f(x_{i-1})}{\\Delta x} \\\\\n",
        "f''(x) = \\frac{f(x_{i})-2f(x_{i-1})+f(x_{i-2})}{\\Delta x^2} \\\\\n",
        "f'''(x) = \\frac{f(x_{i})-3f(x_{i-1})+3f(x_{i-2})-f(x_{i-3})}{\\Delta x^3} \\\\\n",
        "f^{IV}(x) = \\frac{f(x_{i})-4f(x_{i-1})+6f(x_{i-2})-4f(x_{i-3})+f(x_{i-4})}{\\Delta x^4} \\\\ _{\\text{(Eq. 5.3)}}\n",
        "$$\n",
        "Backward Finite Differentiation also has an apprximation error of $O(h)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwsULgPdvUyS"
      },
      "source": [
        "# Backward Definite Differentiation\n",
        "grad1 = (f(x)-f(x-dx))/dx\n",
        "grad2 = (f(x)-2*f(x-dx)+f(x-2*dx))/dx**2\n",
        "print(f'f(0.1) = {f(x)}')\n",
        "print(f'f\\'(0.1) = {grad1}, error @ {abs(f_1(x)-grad1)}')\n",
        "print(f'f\\'\\'(0.1) = {grad2}, error @ {abs(f_2(x)-grad2)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-eE4WTPBkXJ"
      },
      "source": [
        "# Backward Definite Differentiation (General Function)\n",
        "def diff_bwd(f,x,dx,degree=1):\n",
        "  f_ans = f(x+(np.arange(0,-(degree+1),-1))*dx)\n",
        "  bin = binom_coeffs(degree)\n",
        "  bin[1::2] *= -1\n",
        "  diff = (bin @ f_ans) / dx**degree\n",
        "  return diff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I55oQabCDAex"
      },
      "source": [
        "degree = 3\n",
        "for n in range(0,degree+1):\n",
        "  print(diff_bwd(f,x,dx,n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct2QlMerwXlT"
      },
      "source": [
        "## 5.2 Central Finite Differentiation\n",
        "\n",
        "The diffrence between the forward and backward finite differentiation is how $\\Delta y$ is sample. Instead of getting values incrementing from $x$ we take the values preceeding $x$ or simply doing decrements. The formulae below shows the progression.\n",
        "$$f'(x) = \\frac{f(x_{i+1})-f(x_{i-1})}{2\\Delta x} \\\\\n",
        "f''(x) = \\frac{f(x_{i+1})-2f(x_{i})+f(x_{i-1})}{\\Delta x^2} \\\\\n",
        "f'''(x) = \\frac{f(x_{i+2})-3f(x_{i+1})+3f(x_{i-1})-f(x_{i-2})}{2\\Delta x^3} \\\\\n",
        "f^{IV}(x) = \\frac{f(x_{i+2})-4f(x_{i+1})+6f(x_{i})-4f(x_{i-1})+f(x_{i-2})}{\\Delta x^4} \\\\ _{\\text{(Eq. 5.4)}}\n",
        "$$\n",
        "Central Finite Differentiation has an apprximation error of $O(h^2)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJVWy7KNxGFV"
      },
      "source": [
        "# Central Definite Differentiation\n",
        "grad1 = (f(x+dx)-f(x-dx))/(2*dx)\n",
        "grad2 = (f(x+dx)-2*f(x)+f(x-dx))/dx**2\n",
        "print(f'f(0.1) = {f(x)}')\n",
        "print(f'f\\'(0.1) = {grad1}, error @ {abs(f_1(x)-grad1)}')\n",
        "print(f'f\\'\\'(0.1) = {grad2}, error @ {abs(f_2(x)-grad2)}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URjpmUp-04DD"
      },
      "source": [
        "## 5.4 SciPy Derivatives\n",
        "For more complex methods, it is inevitable to used abstracted functions for differentiation. Thus, we will use the derivatives method in solving the derivative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCBpccu4E0-o"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import derivative\n",
        "x = np.arange(0,5)\n",
        "f = lambda x : x**2\n",
        "derivative(f,x,dx=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB-WkJnXE2tg"
      },
      "source": [
        "f = lambda x : np.sin(2*x)-0.05*x**2+np.exp(-2*x)\n",
        "X = np.linspace(0,20)\n",
        "plt.plot(X,f(X), label='f(x)')\n",
        "plt.plot(X,derivative(f,X), label='f\\'(x)')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpiqfXFEzL-j"
      },
      "source": [
        "## 5.5 Taylor Series Expansion\n",
        "A Taylor series is a series expansion of a function about a point. A one-dimensional Taylor series is an expansion of a real function $f(x)$ about $a$ point $x=a$ is given by:\n",
        "$$f(x)=f(a)+f'(a)(x-a)+ \\frac{1}{2!}f''(a)(x-a)^2 + \\frac{1}{3!}f'''(a)(x-a)^3+...+\\frac{1}{n!}f^{(n)}(a)(x-a)^n + ... \\\\ _{\\text{(Eq. 5.5)}}$$\n",
        "\n",
        "The Taylor series can be used to approximate any differentiable function given a power series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM4KTxzu1UwC"
      },
      "source": [
        "# End of Module Activity\n",
        "$\\text{Use another notebook to answer the following problems.}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7LJt4bS1tpL"
      },
      "source": [
        "## Part 1\n",
        "1.) Create a function named `diff_cen` that computes the central definite derivaitve for a function. The parameters of the function should follow the parameter format:\n",
        "\n",
        "`diff_cen(f,x,dx,degree=1)`\n",
        "\n",
        "Whereas:\n",
        "> `f` could be any function\n",
        ">\n",
        "> `x` could be any scalar value as input to the function `f`\n",
        ">\n",
        "> `dx` could be any scalar value for the step\n",
        ">\n",
        "> `degrees` could be any integer representing the degree of the derivative\n",
        "\n",
        "**The use of `scipy.misc.derivative` and other abstracted functions for getting the numerical derivaties are prohibited.**\n",
        "\n",
        "2.) Use the `diff_fwd` and `diff_bwd` and compare the approximation errors for the three differentiation methods. Use the following functions:\n",
        "$$y_1 = \\left(\\frac{4x^2+2x+1}{x+2e^x}\\right)^x\\\\\n",
        "y_2 = \\cos(2x)+\\frac{x^2}{20}+e^{-2x}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tGEvFBU7U3_"
      },
      "source": [
        "## Part 2\n",
        "Research on further concepts and uses of the Taylor Series expansion and implement it at $a=2\\pi$ with $n=7$ for $y_1$ and $y_2$ from Part 1 item 2. You are permitted to use `scipy.misc.derivative` or similar functions for numerical differentiation. Plot the functions and the power series approximating $y_1$ and $y_2$."
      ]
    }
  ]
}