{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NuMeth 4- Curve Fitting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8Ryj5GW/hXLjE1mVu/Nyd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyjdlopez/numeth2021/blob/main/Week%209-13%20-%20Curve%20Fitting%20Techniques/NuMeth_4_Curve_Fitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSW9oeS0e7SY"
      },
      "source": [
        "# Curve Fitting Techniques\n",
        "$_{\\text{Â©D.J. Lopez | 2021 | Computational Methods for Computer Engineers}}$\n",
        "\n",
        "Curve fitting is one of the most used algorithms for optmization expecially in business applications. The use of curve-fitting functions ranges from engineering and signal applications such as approximations and signal replication to business applications in forecasting and operations optimization. In this module, we will discuss the several techniques that can be used in curve fitting. Specifically, we will cover:\n",
        "\n",
        "* Linear Regression\n",
        "* Multiple Linear Regression\n",
        "* Least-Squares Method (Normal Equation Method)\n",
        "* Metrics of Regression\n",
        "* Linear Interpolation\n",
        "* Lagrange Method\n",
        "* Newton's Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIYfC2LKgn5v"
      },
      "source": [
        "## 4.1 Curves\n",
        "When talking about curves, it is not simply wavy lines or simple drawing elements. Rather in our course, we take curves as functions. In the previous lessons we have seen the graphs of the functions and we call them curves as well. But in this module, we are going to identify what is the function based on sets of data. We can use this for idnetifying missing data or creating approximation for new data considering the function we created. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzSpvtTogtcG"
      },
      "source": [
        "### 4.1.1 Extrapolation\n",
        "Extrapolation can be imagined as the appoximation of data beyond the dataset based on the given data,function, or curve describing a specific dataset. One method in data extrapolation that we will discuss in this course is regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMtfpYTegv6-"
      },
      "source": [
        "### 4.1.2 Interpolation\n",
        "Interpolation similar to extrapolation approximates data based on existing data, function, or curve but rather finding data beyond the given set of data it finds more specific or missing data points within a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mg0RPdog0Yu"
      },
      "source": [
        "## 4.2 Extrapolation / Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afg0X7OwhFut"
      },
      "source": [
        "### 4.2.1 Linear Regression\n",
        "As the name suggests, linear regression tries to find the best fit straight line to a given dataset. This algorithm is one of the simplest yet most important alogorithm in regression since it is the foundation of many more complex regression techniques.\n",
        "\n",
        "The goal of this algorithm is finding a linear equation that would best describe a set of data. The equation to be used in finding that linear equation is function is given as:\n",
        "$$y = \\omega_0 + \\omega_1 X \\\\ _{\\text{(Eq. 4.1)}}$$\n",
        "Whereas $X$ is the dataset while $y$ is the corresponding values for each datapoint in $X$. The variable $\\omega$ is called the weight of the dataset consiting of $\\omega_0$ and $\\omega_1$. In other literature, $\\omega_0$ is called the bias term sometimes written as $b$. The following equations are used to solve for $\\omega_0$ and $\\omega_1$:\n",
        "$$\\omega_0 = r\\frac{\\sigma_y}{\\sigma_x}=\\frac{\\bar{y}*\\sum(x_i^2)-\\bar{x}\\sum(x_i*y_i)}{\\sum(x^2_i-n\\bar{x}^2)}\\\\ _{\\text{(Eq. 4.2.1)}}$$\n",
        "$$\\omega_1 =\\bar{y}-\\omega_0\\bar{x}= \\frac{\\sum(x_i*y_i)-\\bar{x}\\sum(y_i)}{\\sum(x^2_i-n\\bar{x}^2)}\\\\ _{\\text{(Eq. 4.2.2)}}$$\n",
        "\n",
        "$$\\omega_0 = r\\frac{\\sigma_y}{\\sigma_x}\\\\ _{\\text{(Eq. 4.2.3)}}$$\n",
        "Whereas $r$ is the Pearson correlation solved as:\n",
        "$$r = \\frac{\\sum((x-\\bar{x})(y-\\bar{y}))}{\\sqrt{\\sum(x-\\bar{x})\\sum(y-\\bar{y})}}\\\\ _{\\text{(Eq. 4.2.4)}}$$\n",
        "\n",
        "$$\\omega_1 =\\bar{y}-\\omega_0\\bar{x} \\\\ _{\\text{(Eq. 4.2.5)}}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aWBqQrWZTJn"
      },
      "source": [
        "'''\n",
        "Since we are going to use datasets for this module, we will be generating dummy \n",
        "data with numpy. We will use as matplotlib for visualizing the results as well.\n",
        "'''\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eePyeJ66bhje"
      },
      "source": [
        "n = 10\n",
        "X = np.arange(0,n,1,dtype=np.float64)\n",
        "\n",
        "m = np.random.uniform(0.4,0.5,(n,))\n",
        "b = np.random.uniform(8,10,(n,))\n",
        "\n",
        "y = m*X+b \n",
        "\n",
        "print(f\"X: {X}\")\n",
        "print(f\"y: {y}\")\n",
        "print(f\"w1 approx = {m.mean()},w0 approx. = {b.mean()}\")\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.grid()\n",
        "plt.scatter(X,y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YsgSBrHnWd2"
      },
      "source": [
        "def linear_regressor(X,y):\n",
        "  X = np.array(X)\n",
        "  y = np.array(y)\n",
        "  n = X.size\n",
        "  w0 = (y.mean()*np.sum(X**2)-X.mean()*np.sum(X*y)) / (np.sum(X**2) - n*X.mean()**2)\n",
        "  w1 = (np.sum(X*y) - X.mean()*np.sum(y)) / (np.sum(X**2) - n*X.mean()**2)\n",
        "  return w0,w1\n",
        "w0,w1 = linear_regressor(X,y)\n",
        "print(\"Linear Regression Equation: y = {:.3f}x + {:.3f}\".format(w1, w0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKp1iagOwrUG"
      },
      "source": [
        "## Plotting the Regression line\n",
        "def show_regline(X,y,w1,w0):\n",
        "  x_min, x_max = X.min() - 1, X.max() + 1\n",
        "  linex = np.linspace(x_min, x_max)\n",
        "  liney = w1*linex+w0\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.grid()\n",
        "  plt.scatter(X,y)\n",
        "  plt.plot(linex, liney, c='red')\n",
        "  plt.show()\n",
        "show_regline(X,y,w1,w0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nYumocqwDFM"
      },
      "source": [
        "def lin_reg(val,w0,w1):\n",
        "  return w1*val + w0 #model\n",
        "print(lin_reg(10, w0, w1))\n",
        "X_new, y_new = X.copy(), y.copy()\n",
        "for i in range(10,16):\n",
        "  X_new = np.insert(X_new,-1, i)\n",
        "  y_new = np.insert(y_new,-1, lin_reg(i,w0,w1))\n",
        "show_regline(X_new, y_new, w1, w0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh-gaR47zVw8"
      },
      "source": [
        "np.random.seed(100)\n",
        "X_1 = np.arange(0, 20, 1)\n",
        "y_1 = X_1 - 2 * (X_1 ** 2) + 0.5 * (X_1 ** 3) + np.random.normal(-3, 3, 20)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.grid()\n",
        "plt.scatter(X_1, y_1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoqKojTiz3fe"
      },
      "source": [
        "w0_q,w1_q = linear_regressor(X_1, y_1)\n",
        "show_regline(X_1,y_1,w0_q,w1_q) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPC1enDXhH9F"
      },
      "source": [
        "### 4.2.2 Multiple Linear Regression\n",
        "Multiple linear regression, as the name suggests uses more linear regressors in the algorithm. This can be used if there are more than one features to a dataset. The MLR can be formulated as:\n",
        "$$y = \\omega_0 + \\omega_1 x_1 + \\omega_2 x_2 + ... + \\omega_n x_n \\\\ _{\\text{(Eq. 4.4)}}$$\n",
        "Whereas $\\omega_0$ is the bias term while $\\omega_n$ are the weights or slopes of the features $x_n$. The simplest way to implement an MLR algorithm is looping over each feature and their dataset and compute the corresponding weights. In this course, we are going to implement vectorization in implementing MLR. So instead of hte linear equation in Eq. 4.4 we can re-form the equation to the matrix equation:\n",
        "$$y = \\omega X^T$$\n",
        "Whereas $\\omega$ is a vector that includes all the weights of the features $\\begin{bmatrix}\\omega_0 \\\\ \\omega_1 \\\\ \\omega_2 \\\\ \\vdots \\\\ \\omega_n\\end{bmatrix}$. While $X$ are the data of each feature vector $\\begin{bmatrix}1\\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix}$.\n",
        "\n",
        "We will use the **Normal Equation** in solving MLR. The Normal equation uses the Least-Squares Cost function and is formulated as:\n",
        "$$\\theta = (X^TX)^{-1}X^Ty \\\\ _{\\text{(Eq. 4.5)}}$$\n",
        "Whereas $\\theta$ is the hypothesis or model to be created while $X$ represents the data vector and $y$ represents the labels or values corresponding to the data vector. The term $(X^TX)^{-1}$ is called the **pseudoinverse** or the **Moore-Penrose** matrix. The pseudoinverse of a matrix term of Eq. 4.5 assures that the data are normal or orthogonal. This helps check the property of Autocorrelation between the features of the data. The other properties of datasets that are safe for linear regression are Homoscedasticity, Non-multicollinearity, and Non-endogeneity. These properties will be discussed in depth in the Machine Learning Course of the AIDA Electives. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOQVgwuxDwYN"
      },
      "source": [
        "X = np.array([\n",
        "              [1,2,3],\n",
        "              [7,3,2],\n",
        "              [9,6,10],\n",
        "])\n",
        "y = np.array([[4,3,8]]).T\n",
        "bias = np.ones(y.shape)\n",
        "X_train = np.append(bias,X, axis=1).T\n",
        "X_dot = X_train @ X_train.T\n",
        "pseudoinv = np.linalg.inv(X_dot)\n",
        "y_dot = X_train @ y\n",
        "theta = pseudoinv @ y_dot\n",
        "for i in range(len(theta)):\n",
        "  print(f\"w{i} : {float(theta[i])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlLDXLtjhrdN"
      },
      "source": [
        "### 4.2.3 Metrics of Regression\n",
        "For us to determine how regression models are reliable or accurate we can use the following statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVyZd4wghvou"
      },
      "source": [
        "#### Measures of Reliability\n",
        "Measures of reliability or predictability tells how models are reliable for predicting new values. Some of statistics used here are the R-Squared and the Adjusted R-Squared."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63vM_UVOajEt"
      },
      "source": [
        "##### *R-Squared ($R^2$)*\n",
        "Represents the proportion of the variance for a prediction that is explained by the inputs in a regression model. The formula is given as:\n",
        "\n",
        "$$\\text{R}^2 = 1 - \\frac{\\sum(y-\\hat{y})^2}{\\sum(y-\\bar{y})^2} \\\\ _{\\text{(Eq. 4.6)}}$$\n",
        "Whereas the numerator for the rational part is called the **residual of the sum of squares** in which $\\hat{y}$ is the prediction of the model and $y$ is from the testing dataset. The denominator of the rational part is called the **total sum of squares**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jelUoFBRclDM"
      },
      "source": [
        "###### *Adjusted R-Squared ($\\text{Adj }R^2$)*\n",
        "A modified version of the R-squared which has been adjusted to the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. The formula is given as:\n",
        "$$\\text{Adj. R}^2 = 1 - \\begin{bmatrix}\\frac{(1-\\text{R}^2)(n-1)}{n-p-1}\\end{bmatrix} \\\\ _{\\text{(Eq. 4.6)}}$$\n",
        "Whereas $n$ is the size of the sample and $p$ is the number of predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txprQifkh_Q2"
      },
      "source": [
        "#### Measures of Error \n",
        "Measures of error can tell how \"off\" predicted values are from the true values or ground truth. These stastical measures can also serve as a minimisation cost function in optimizing a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca6XMLt7eQq7"
      },
      "source": [
        "##### *Mean Squared Error (MSE)*\n",
        "The MSE shows an estimation of the deviations of the predictions from the ground truths by getting the average of the squared errors. It can be also interpreted as the mean of the Euclidean distances of the predictions and ground truths. MSE is best when considering the existence of outliers in the data. The formula is given as:\n",
        "$$\\\\ \\text{MSE}=\\frac{1}{n}\\sum(y-\\hat{y})^2 \\\\ _{\\text{(Eq. 4.7)}}$$  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hZ4JPEteUly"
      },
      "source": [
        "##### *Root Mean Squared Error (MSE)*\n",
        "The limitations of MSE as a measure of measure is its intepretability wherein it does not express the error in the original measurement units. The RMSE can also be considered as the standard deviation of the residuals unlike the MSE which is the variance. The formula of is given as:\n",
        "$$\\text{RMSE}= \\sqrt{\\text{MSE}} \\\\ _{\\text{(Eq. 4.8)}}$$  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoWLXK8ZeV_j"
      },
      "source": [
        "##### *Mean Absolute Error (MAE)*\n",
        "The MAE, as its name suggests, takes the average of the Manhattan distances of the predictions and the ground truths. If outliers are not much of a concern for the problem MAE can be a better choice than MSE and RMSE. The formula is given as:\n",
        "$$\\\\ \\text{MAE}=\\frac{1}{n}\\sum{|y-\\hat{y}|} \\\\ _{\\text{(Eq. 4.9)}}$$  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W6gawaQhLsV"
      },
      "source": [
        "### 4.2.4 Applied Uses of Linear Regression\n",
        "Refered discussion: [Applied Linear Regression](https://colab.research.google.com/github/dyjdlopez/numeth2021/blob/main/Week%209-13%20-%20Curve%20Fitting%20Techniques/NuMeth_4_5_Applied_Linear_Regression.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WROprEbQh5xV"
      },
      "source": [
        "## 4.3 Interpolation\n",
        "Interpolation as previously discussed, pertains to the approximation of values within a given range. It is unlike regression or extrapolation that is trying to approximate values beyond the range. Interpolation can be used to increase the resolution of values between such ranges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHOHkoN2iLIs"
      },
      "source": [
        "### 4.3.1 Linear Interpolation\n",
        "This method is the simplest implementation of interpolation considering the modified midpoint formula. Like linear regression, this method is best for linear equations but would have inaccurate approximations for polynomials with higher degrees. The formula is given as:\n",
        "\n",
        "$$ y = y_1 + \\frac{y_2-y_1}{x_2-x_1}(x-x_1) \\\\ _{\\text{(Eq. 4.9)}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ecjZ0HkhLQ"
      },
      "source": [
        "np.random.seed(30)\n",
        "X_2 = np.arange(0, 8, 1, dtype=float)\n",
        "y_2 = X_2 + 4*(X_2 ** 2) - 3*np.random.normal(0, 3, X_2.size)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.grid()\n",
        "plt.scatter(X_2, y_2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cisTTUdflSmL"
      },
      "source": [
        "We can see in this sample that there is a big gap between 5 and 6 of the independent variables. We can apply linear interpolation in bridging the gap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8LuecJklqJB"
      },
      "source": [
        "def lin_interp(x, x1, x2, y1, y2): \n",
        "  return y1 + ((y2-y1)/(x2-x1)) * (x-x1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUOCKT7rnDKx"
      },
      "source": [
        "y_56 = lin_interp(2.5, X_2[2], X_2[3], y_1[2], y_2[3])\n",
        "print(y_56)\n",
        "X_2new = X_2.copy()\n",
        "y_2new = y_2.copy()\n",
        "X_2new = np.insert(X_2new, 3, 2.5)\n",
        "y_2new = np.insert(y_2new, 3, y_25)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.grid()\n",
        "plt.scatter(X_2new, y_2new)\n",
        "plt.plot(X_2new, y_2new)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAjy1DIFpGkE"
      },
      "source": [
        "## If we want to increase the resolution of the graph we can perform linear interpolation for every datapoint\n",
        "## We need to make a routine using the formula we created.\n",
        "inp = 0\n",
        "X_2new = X_2.copy()\n",
        "y_2new = y_2.copy()\n",
        "for i, xi in enumerate(X_2):\n",
        "  if i !=0:\n",
        "    xi -= 0.5\n",
        "    y = lin_interp(xi, X_2[i-1], X_2[i], y_2[i-1], y_2[i])\n",
        "    print(xi, y)\n",
        "    X_2new = np.insert(X_2new, 2*i-1, xi)\n",
        "    y_2new = np.insert(y_2new, 2*i-1, y)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.grid()\n",
        "plt.scatter(X_2, y_2)\n",
        "plt.plot(X_2new, y_2new)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kevpanr1iOX-"
      },
      "source": [
        "### 4.3.2 Lagrange Method\n",
        "The Lagrange method is based on creating a polynomial of degree $n-1$. The degree is dependent on the number of points considered in the dataset $n$. It can be characterized as:\n",
        "$$ y(x) = P_1(x)y_1 + P_2(x)y_3 + P_3(x)y_3 + ... + P_n(x)y_n \\\\ _{\\text{(Eq. 4.10)}}$$\n",
        "This can also be expressed as:\n",
        "$$ y(x) = \\sum_{i=0}^n P_i(x)y_i \\\\ _{\\text{(Eq. 4.11)}}$$\n",
        "Whereas $P(x)$ is the function for the lagrangian polynomial coefficient. Formulated as:\n",
        "$$ P_i(x) = \\prod_{j=0 \\\\ j\\neq i}^n \\frac{(x-x_j)}{(x_i-x_j)} \\\\ _{\\text{(Eq. 4.12)}}$$\n",
        "Eq. 4.11 can then be re-formulated as:\n",
        "$$ y(x) = \\sum_{i=0}^n  y_i \\begin{pmatrix}\\prod_{j=0 \\\\ j\\neq i}^n \\frac{(x-x_j)}{(x_i-x_j)} \\end{pmatrix} \\\\ _{\\text{(Eq. 4.13)}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL81hSbR0-zX"
      },
      "source": [
        "def coeff(x,i,X):\n",
        "  x_temp = np.delete(X,i)\n",
        "  return ((x-x_temp)/(X[i]-x_temp)).prod()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW_gaF0Dvq7t"
      },
      "source": [
        "x = 0.5\n",
        "for i in range(X_2.size):\n",
        "  Pi = coeff(x,i,X_2)\n",
        "  print(Pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekWv9B6s18w4"
      },
      "source": [
        "def lagrange(x,Y,X):\n",
        "  y = 0\n",
        "  for i in range(X.size):\n",
        "    y += Y[i]*coeff(x,i,X)\n",
        "  return y  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-gfkYTk-D2x"
      },
      "source": [
        "lagrange(0.5, y_2, X_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGNCjojT-LoK"
      },
      "source": [
        "X_3new = X_2.copy()\n",
        "y_3new = y_2.copy()\n",
        "for i, xi in enumerate(X_2):\n",
        "  if i !=0:\n",
        "    xi -= 0.5\n",
        "    y = lagrange(xi,y_3new,X_3new)\n",
        "    X_3new = np.insert(X_3new, 2*i-1, xi)\n",
        "    y_3new = np.insert(y_3new, 2*i-1, y)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.grid()\n",
        "# plt.scatter(X_3new, y_3new)\n",
        "\n",
        "plt.plot(X_3new, y_3new, label=\"Lagrange\")\n",
        "plt.plot(X_2new, y_2new, label=\"Linear\", color='green')\n",
        "plt.scatter(X_2, y_2, color='red')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(X_3new)\n",
        "print(y_3new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GUdL5AViSAV"
      },
      "source": [
        "### 4.3.3 Newton's Method\n",
        "The Newton's method can be applied to datapoints to obtain Newton's polynomial. Unlike Lagrange's Method, In Newton's Method, when more data points are to be used, additional basis polynomials and the corresponding coefficients can be calculated, while all existing basis polynomials and their coefficients remain unchanged. Due to the additional terms, the degree of interpolation polynomial is higher and the approximation error may be reduced. This can be used when the interval difference is not same for all sequence of values. This method Newton's polynomial is in the form:\n",
        "$$y(x) = a_0 + (x-x_1)a_1 + (x-x_1)(x-x_2)a_2 + ... + (x-x_1)(x-x_2)...*(x-x_n)a_n \\\\_{\\text{Eq.4.14}}$$\n",
        "The two steps in obtaining the polynomial are:\n",
        "1. Dividing the Differences\n",
        "2. Substitution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP8NIux6L7-8"
      },
      "source": [
        "#### 4.3.3.1 Dividing the Differences\n",
        "This step is done to obtain the coefficients of the polynomials. These coeefficients are the $a_i$ from Eq. 4.14.\n",
        "The divided differences are applied to create a table of values whereas column indicate the degree of the polynomial ($n$) plus 1. While the rows described by the datapoints ($x_i$). For example of a cubic polynomial with 4 datapoints:\n",
        "\n",
        "<table style=\"width:200%\">\n",
        "<tr><th>----(0)-----</th><th>----(1)-----</th><th>----(2)-----</th><th>----(3)-----</th><th>----(4)-----</th>\n",
        "</tr>\n",
        "<tr><td>$x_1$</td><td>$y_1^{(1)}=y_1$</td></tr>\n",
        "<tr><td>$x_2$</td><td>$y_2^{(1)}=y_2$</td><td>$y_2^{(2)}$</td></tr>\n",
        "<tr><td>$x_3$</td><td>$y_3^{(1)}=y_3$</td><td>$y_3^{(2)}$</td><td>$y_3^{(3)}$</td></tr>\n",
        "<tr><td>$x_4$</td><td>$y_4^{(1)}=y_4$</td><td>$y_4^{(2)}$</td><td>$y_4^{(3)}$</td><td>$y_4^{(4)}$</td></tr>\n",
        "</table>\n",
        "The general equation to be used in deriving each $y_i$ is formulated as:\n",
        "$$y_i^{(j+1)} = \\frac{y_i^{(j)}-y^{(j)}}{x_i - x_j}, \\text{for: }j = \\{0,1,2,..n\\} \\text{ and: } i = \\{j+1,...,n+1\\} \\\\_{\\text{Eq. 4.15}}$$\n",
        "The coefficients $a$ can be obtained from the main diagonal of the table, such that $a = \\{y_i^{(j)} | i=j\\} $\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ikaYzB2ZQmg"
      },
      "source": [
        "n = X_2.size\n",
        "y_temp = np.zeros((n, n))\n",
        "y_temp[:,0] = y_2\n",
        "for j in range(n-1):\n",
        "  for i in range(j+1, n):\n",
        "    y_temp[i, j+1] = (y_temp[i,j]-y_temp[j,j])/(X_2[i] - X_2[j])\n",
        "print(y_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN_hT6adcyio"
      },
      "source": [
        "def newton_coeff(X,y):\n",
        "  n = X.size\n",
        "  y_temp = np.zeros((n, n))\n",
        "  y_temp[:,0] = y\n",
        "  for j in range(n-1):\n",
        "    for i in range(j+1, n):\n",
        "      y_temp[i, j+1] = (y_temp[i,j]-y_temp[j,j])/(X[i] - X[j])\n",
        "  a = np.diag(y_temp)\n",
        "  return y_temp, a\n",
        "\n",
        "newton_coeff(X_2,y_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDaB78gJYEKu"
      },
      "source": [
        "#### 4.3.3.2 Substitution\n",
        "For the last step, the polynomial is calculated for a given $x$ value in Eq. 4.14. We can re-formulate Eq. 4.14 into its general form by:\n",
        "$$y(x) = a_0 + \\sum^n_{i=0}\\begin{bmatrix}\\prod^i_{j=1}(x-x_j)\\end{bmatrix}a_i$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-kpnWqmf1oP"
      },
      "source": [
        "### Newton coeff matrix\n",
        "xp = 5.6\n",
        "_, a = newton_coeff(X_2,y_2)\n",
        "coeff_mat = np.zeros(n)\n",
        "for i in range(0,n):  \n",
        "  coeff_mat[i] = 1 if i==0 else np.product(xp-X_2[:i])\n",
        "yp = a @ coeff_mat\n",
        "yp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34xElHNZj98l"
      },
      "source": [
        "def newton_interp(xp,y,X):\n",
        "  n = X.size\n",
        "  _, a = newton_coeff(X,y)\n",
        "  coeff_mat = np.zeros(n)\n",
        "  for i in range(0,n):  \n",
        "    coeff_mat[i] = 1 if i==0 else np.product(xp-X[:i])\n",
        "  return a @ coeff_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM-ki__WkQTB"
      },
      "source": [
        "X_4new = X_2.copy()\n",
        "y_4new = y_2.copy()\n",
        "for i, xi in enumerate(X_2):\n",
        "  if i !=0:\n",
        "    xi -= 0.5\n",
        "    y = newton_interp(xi,y_4new,X_4new)\n",
        "    X_4new = np.insert(X_4new, 2*i-1, xi)\n",
        "    y_4new = np.insert(y_4new, 2*i-1, y)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.grid()\n",
        "\n",
        "plt.plot(X_4new, y_4new, label=\"Newton\", color=\"purple\")\n",
        "# plt.plot(X_3new, y_3new, label=\"Lagrange\", color=\"blue\")\n",
        "plt.plot(X_2new, y_2new, label=\"Linear\", color='green')\n",
        "plt.scatter(X_2, y_2, color='red')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}